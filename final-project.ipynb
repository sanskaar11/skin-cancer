{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"tpu1vmV38","dataSources":[{"sourceId":104884,"sourceType":"datasetVersion","datasetId":54339},{"sourceId":1418,"sourceType":"modelInstanceVersion","modelInstanceId":1199},{"sourceId":3242,"sourceType":"modelInstanceVersion","modelInstanceId":2405}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nfrom glob import glob\nfrom tqdm import tqdm\n\nimport cv2\nimport numpy as np\nimport pandas as pd\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\n%matplotlib inline\nplt.ion()   # interactive mode\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\nimport torchvision\nfrom torchvision import transforms, models\nfrom torch.utils.data import DataLoader, Dataset\n\n# sklearn libraries\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.status.busy":"2023-12-03T15:21:32.066946Z","iopub.execute_input":"2023-12-03T15:21:32.067655Z","iopub.status.idle":"2023-12-03T15:21:32.091723Z","shell.execute_reply.started":"2023-12-03T15:21:32.067620Z","shell.execute_reply":"2023-12-03T15:21:32.090820Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"os.listdir('../input/skin-cancer-mnist-ham10000')","metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","execution":{"iopub.status.busy":"2023-12-03T15:21:34.479124Z","iopub.execute_input":"2023-12-03T15:21:34.480265Z","iopub.status.idle":"2023-12-03T15:21:34.487559Z","shell.execute_reply.started":"2023-12-03T15:21:34.480219Z","shell.execute_reply":"2023-12-03T15:21:34.486192Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"base_dir = os.path.join('..', 'input','skin-cancer-mnist-ham10000' )\nall_image_path = glob(os.path.join(base_dir, '*', '*.jpg'))\nimageid_path_dict = {os.path.splitext(os.path.basename(x))[0]: x for x in all_image_path}\nlesion_type_dict = {\n    'nv': 'Melanocytic nevi',\n    'mel': 'dermatofibroma',\n    'bkl': 'Benign keratosis-like lesions ',\n    'bcc': 'Basal cell carcinoma',\n    'akiec': 'Actinic keratoses',\n    'vasc': 'Vascular lesions',\n    'df': 'Dermatofibroma'\n}\n","metadata":{"execution":{"iopub.status.busy":"2023-12-03T15:21:35.727071Z","iopub.execute_input":"2023-12-03T15:21:35.727961Z","iopub.status.idle":"2023-12-03T15:21:35.868552Z","shell.execute_reply.started":"2023-12-03T15:21:35.727925Z","shell.execute_reply":"2023-12-03T15:21:35.867438Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"w, h = 10, 10\nfig = plt.figure(figsize=(15, 15))\ncolumns, rows = 3, 2\nstart, end = 0, len(imageid_path_dict)\nax = []\nimport random\nfor i in range(columns*rows):\n    k = random.randint(start, end)\n    img = mpimg.imread((all_image_path[k]))\n#     title = (train_df.iloc[k,0]).split('/')\n#     title = title[2]+'-'+title[3]+'-'+title[4]\n#     k += 1\n    # create subplot and append to ax\n    ax.append( fig.add_subplot(rows, columns, i+1) )\n#     ax[-1].set_title(title)  # set title\n    plt.xticks([])\n    plt.yticks([])\n    plt.imshow(img, cmap=\"gray\")\nplt.tight_layout()\nplt.show()  # finally, render the plot","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2023-12-03T15:21:36.560827Z","iopub.execute_input":"2023-12-03T15:21:36.561176Z","iopub.status.idle":"2023-12-03T15:21:38.465370Z","shell.execute_reply.started":"2023-12-03T15:21:36.561148Z","shell.execute_reply":"2023-12-03T15:21:38.464352Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Checking the size and number of channels in the image\narr = np.asarray(Image.open(all_image_path[48]))\narr.shape","metadata":{"execution":{"iopub.status.busy":"2023-12-03T15:21:38.466798Z","iopub.execute_input":"2023-12-03T15:21:38.467104Z","iopub.status.idle":"2023-12-03T15:21:38.482124Z","shell.execute_reply.started":"2023-12-03T15:21:38.467079Z","shell.execute_reply":"2023-12-03T15:21:38.481260Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_original = pd.read_csv(os.path.join(base_dir, 'HAM10000_metadata.csv'))\ndf_original['path'] = df_original['image_id'].map(imageid_path_dict.get)\ndf_original['cell_type'] = df_original['dx'].map(lesion_type_dict.get)\ndf_original['cell_type_idx'] = pd.Categorical(df_original['cell_type']).codes\ndf_original.head()\n","metadata":{"execution":{"iopub.status.busy":"2023-12-03T15:21:41.642008Z","iopub.execute_input":"2023-12-03T15:21:41.642421Z","iopub.status.idle":"2023-12-03T15:21:41.686796Z","shell.execute_reply.started":"2023-12-03T15:21:41.642387Z","shell.execute_reply":"2023-12-03T15:21:41.685797Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Dx is short for diagnosis (for the patient) and dx_type is how the diagnosis was made.\nA bit more about each type of diagnosis and how they were made is available in the original paper https://arxiv.org/abs/1803.10417","metadata":{}},{"cell_type":"code","source":"df_original[['cell_type_idx', 'cell_type']].sort_values('cell_type_idx').drop_duplicates()","metadata":{"execution":{"iopub.status.busy":"2023-12-03T15:21:46.031478Z","iopub.execute_input":"2023-12-03T15:21:46.032272Z","iopub.status.idle":"2023-12-03T15:21:46.048420Z","shell.execute_reply.started":"2023-12-03T15:21:46.032240Z","shell.execute_reply":"2023-12-03T15:21:46.047404Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In the table above we have a nice overview of our groundtruth data. What we care for is only the column 'cell_type_idx', because these values are needed for the model training. \nBut of course it is nice to know what these labels mean, that is what the column 'cell_type' is for. Lets quickly check how often the different tumors occur in our dataset:","metadata":{}},{"cell_type":"code","source":"df_original['cell_type'].value_counts()\n","metadata":{"execution":{"iopub.status.busy":"2023-12-03T15:21:48.033579Z","iopub.execute_input":"2023-12-03T15:21:48.034342Z","iopub.status.idle":"2023-12-03T15:21:48.042724Z","shell.execute_reply.started":"2023-12-03T15:21:48.034310Z","shell.execute_reply":"2023-12-03T15:21:48.041727Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def compute_img_mean_std(image_paths):\n    \"\"\"\n        computing the mean and std of three channel on the whole dataset,\n        first we should normalize the image from 0-255 to 0-1\n    \"\"\"\n\n    img_h, img_w = 224, 224\n    imgs = []\n    means, stdevs = [], []\n\n    for i in tqdm(range(len(image_paths))):\n        img = cv2.imread(image_paths[i])\n        img = cv2.resize(img, (img_h, img_w))\n        imgs.append(img)\n\n    imgs = np.stack(imgs, axis=3)\n    print(imgs.shape)\n\n    imgs = imgs.astype(np.float32) / 255.\n\n    for i in range(3):\n        pixels = imgs[:, :, i, :].ravel()  # resize to one row\n        means.append(np.mean(pixels))\n        stdevs.append(np.std(pixels))\n\n    means.reverse()  # BGR --> RGB\n    stdevs.reverse()\n\n    print(\"normMean = {}\".format(means))\n    print(\"normStd = {}\".format(stdevs))\n    return means,stdevs","metadata":{"execution":{"iopub.status.busy":"2023-12-03T15:21:49.892409Z","iopub.execute_input":"2023-12-03T15:21:49.893340Z","iopub.status.idle":"2023-12-03T15:21:49.901556Z","shell.execute_reply.started":"2023-12-03T15:21:49.893306Z","shell.execute_reply":"2023-12-03T15:21:49.900590Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# norm_mean,norm_std = compute_img_mean_std(all_image_path)","metadata":{"execution":{"iopub.status.busy":"2023-12-03T15:21:50.838739Z","iopub.execute_input":"2023-12-03T15:21:50.839376Z","iopub.status.idle":"2023-12-03T15:21:50.843669Z","shell.execute_reply.started":"2023-12-03T15:21:50.839346Z","shell.execute_reply":"2023-12-03T15:21:50.842563Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"normMean = [0.7630358, 0.54564357, 0.5700475]\nnormStd = [0.14092763, 0.15261263, 0.16997081]\n# for future reference","metadata":{"execution":{"iopub.status.busy":"2023-12-03T15:21:56.297343Z","iopub.execute_input":"2023-12-03T15:21:56.298144Z","iopub.status.idle":"2023-12-03T15:21:56.302745Z","shell.execute_reply.started":"2023-12-03T15:21:56.298110Z","shell.execute_reply":"2023-12-03T15:21:56.301563Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# this will tell us how many images are associated with each lesion_id\ndf_undup = df_original.groupby('lesion_id').count()\n# now we filter out lesion_id's that have only one image associated with it\ndf_undup = df_undup[df_undup['image_id'] == 1]\ndf_undup.reset_index(inplace=True)\ndf_undup.head()","metadata":{"execution":{"iopub.status.busy":"2023-12-03T15:22:00.857969Z","iopub.execute_input":"2023-12-03T15:22:00.858344Z","iopub.status.idle":"2023-12-03T15:22:00.894956Z","shell.execute_reply.started":"2023-12-03T15:22:00.858313Z","shell.execute_reply":"2023-12-03T15:22:00.893912Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# here we identify lesion_id's that have duplicate images and those that have only one image.\ndef get_duplicates(x):\n    unique_list = list(df_undup['lesion_id'])\n    if x in unique_list:\n        return 'unduplicated'\n    else:\n        return 'duplicated'\n\n# create a new colum that is a copy of the lesion_id column\ndf_original['duplicates'] = df_original['lesion_id']\n# apply the function to this new column\ndf_original['duplicates'] = df_original['duplicates'].apply(get_duplicates)\ndf_original.head()","metadata":{"execution":{"iopub.status.busy":"2023-12-03T15:22:01.832116Z","iopub.execute_input":"2023-12-03T15:22:01.832472Z","iopub.status.idle":"2023-12-03T15:22:09.501189Z","shell.execute_reply.started":"2023-12-03T15:22:01.832445Z","shell.execute_reply":"2023-12-03T15:22:09.500210Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_original['duplicates'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2023-12-03T15:22:09.503092Z","iopub.execute_input":"2023-12-03T15:22:09.503408Z","iopub.status.idle":"2023-12-03T15:22:09.511699Z","shell.execute_reply.started":"2023-12-03T15:22:09.503381Z","shell.execute_reply":"2023-12-03T15:22:09.510781Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# now we filter out images that don't have duplicates\ndf_undup = df_original[df_original['duplicates'] == 'unduplicated']\ndf_undup.shape","metadata":{"execution":{"iopub.status.busy":"2023-12-03T15:22:09.512987Z","iopub.execute_input":"2023-12-03T15:22:09.513301Z","iopub.status.idle":"2023-12-03T15:22:09.526789Z","shell.execute_reply.started":"2023-12-03T15:22:09.513263Z","shell.execute_reply":"2023-12-03T15:22:09.525682Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# now we create a val set using df because we are sure that none of these images have augmented duplicates in the train set\ny = df_undup['cell_type_idx']\n_, df_val = train_test_split(df_undup, test_size=0.2, random_state=101, stratify=y)\ndf_val.shape","metadata":{"execution":{"iopub.status.busy":"2023-12-03T15:22:11.178984Z","iopub.execute_input":"2023-12-03T15:22:11.179775Z","iopub.status.idle":"2023-12-03T15:22:11.200442Z","shell.execute_reply.started":"2023-12-03T15:22:11.179743Z","shell.execute_reply":"2023-12-03T15:22:11.199547Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_val['cell_type_idx'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2023-12-03T15:22:13.020764Z","iopub.execute_input":"2023-12-03T15:22:13.021753Z","iopub.status.idle":"2023-12-03T15:22:13.031132Z","shell.execute_reply.started":"2023-12-03T15:22:13.021715Z","shell.execute_reply":"2023-12-03T15:22:13.030007Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# This set will be df_original excluding all rows that are in the val set\n# This function identifies if an image is part of the train or val set.\ndef get_val_rows(x):\n    # create a list of all the lesion_id's in the val set\n    val_list = list(df_val['image_id'])\n    if str(x) in val_list:\n        return 'val'\n    else:\n        return 'train'\n\n# identify train and val rows\n# create a new colum that is a copy of the image_id column\ndf_original['train_or_val'] = df_original['image_id']\n# apply the function to this new column\ndf_original['train_or_val'] = df_original['train_or_val'].apply(get_val_rows)\n# filter out train rows\ndf_train = df_original[df_original['train_or_val'] == 'train']\nprint(len(df_train))\nprint(len(df_val))\n","metadata":{"execution":{"iopub.status.busy":"2023-12-03T15:22:14.036101Z","iopub.execute_input":"2023-12-03T15:22:14.036465Z","iopub.status.idle":"2023-12-03T15:22:15.690319Z","shell.execute_reply.started":"2023-12-03T15:22:14.036438Z","shell.execute_reply":"2023-12-03T15:22:15.689338Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train['cell_type'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2023-12-03T15:22:15.692148Z","iopub.execute_input":"2023-12-03T15:22:15.692538Z","iopub.status.idle":"2023-12-03T15:22:15.700948Z","shell.execute_reply.started":"2023-12-03T15:22:15.692484Z","shell.execute_reply":"2023-12-03T15:22:15.699956Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_val['cell_type'].value_counts()\n","metadata":{"execution":{"iopub.status.busy":"2023-12-03T15:22:16.775829Z","iopub.execute_input":"2023-12-03T15:22:16.776192Z","iopub.status.idle":"2023-12-03T15:22:16.784239Z","shell.execute_reply.started":"2023-12-03T15:22:16.776163Z","shell.execute_reply":"2023-12-03T15:22:16.783227Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From From the above statistics of each category, we can see that there is a serious class imbalance in the training data. To solve this problem, I think we can start from two aspects, one is equalization sampling, and the other is a loss function that can be used to mitigate category imbalance during training, such as focal loss.","metadata":{}},{"cell_type":"code","source":"# Copy fewer class to balance the number of 7 classes\ndata_aug_rate = [15,10,5,50,0,40,5]\nfor i in range(7):\n    if data_aug_rate[i]:\n        df_train = pd.concat([df_train] + [df_train.loc[df_train['cell_type_idx'] == i, :].copy() for _ in range(data_aug_rate[i] - 1)], ignore_index=True)\n\ndf_train['cell_type'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2023-12-03T15:22:18.984164Z","iopub.execute_input":"2023-12-03T15:22:18.984533Z","iopub.status.idle":"2023-12-03T15:22:19.140573Z","shell.execute_reply.started":"2023-12-03T15:22:18.984487Z","shell.execute_reply":"2023-12-03T15:22:19.139464Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train = df_train.reset_index()\ndf_val = df_val.reset_index()","metadata":{"execution":{"iopub.status.busy":"2023-12-03T15:22:21.739962Z","iopub.execute_input":"2023-12-03T15:22:21.740343Z","iopub.status.idle":"2023-12-03T15:22:21.756293Z","shell.execute_reply.started":"2023-12-03T15:22:21.740314Z","shell.execute_reply":"2023-12-03T15:22:21.755460Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.sample(5)","metadata":{"execution":{"iopub.status.busy":"2023-12-03T15:22:24.027701Z","iopub.execute_input":"2023-12-03T15:22:24.028439Z","iopub.status.idle":"2023-12-03T15:22:24.046474Z","shell.execute_reply.started":"2023-12-03T15:22:24.028407Z","shell.execute_reply":"2023-12-03T15:22:24.045469Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ndevice","metadata":{"execution":{"iopub.status.busy":"2023-12-03T15:22:25.077573Z","iopub.execute_input":"2023-12-03T15:22:25.077943Z","iopub.status.idle":"2023-12-03T15:22:25.164299Z","shell.execute_reply.started":"2023-12-03T15:22:25.077915Z","shell.execute_reply":"2023-12-03T15:22:25.163139Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class PretrainedResNext(nn.Module):\n    def __init__(self, num_class=7):\n        super().__init__()\n        resNext = models.resnext101_32x8d(pretrained=True)\n        self.channels = resNext.fc.out_features\n        for params in resNext.parameters():\n            params.requires_grad_(False)\n        self.features = nn.Sequential(*list(resNext.children()))\n        self.relu = nn.ReLU(inplace=True)\n        self.fc1 = nn.Linear(self.channels, num_class)\n        self.softmax = nn.Softmax()\n\n    def forward(self, x):\n        features = self.features(x)\n        out = self.relu(features)\n        out = nn.functional.adaptive_avg_pool2d(out, (1, 1))\n        out = out.view(-1, self.channels)\n        out = self.fc1(out)\n        out = self.SoftMax(out)\n        return out","metadata":{"execution":{"iopub.status.busy":"2023-12-03T15:22:26.117872Z","iopub.execute_input":"2023-12-03T15:22:26.118738Z","iopub.status.idle":"2023-12-03T15:22:26.128983Z","shell.execute_reply.started":"2023-12-03T15:22:26.118699Z","shell.execute_reply":"2023-12-03T15:22:26.127914Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = PretrainedResNext()\nmodel_ft = models.resnext101_32x8d(pretrained=True)\nmodel_ft.fc = nn.Linear(in_features=2048, out_features=7)\nmodel_ft\nmodel = model_ft","metadata":{"execution":{"iopub.status.busy":"2023-12-03T15:22:28.848539Z","iopub.execute_input":"2023-12-03T15:22:28.848931Z","iopub.status.idle":"2023-12-03T15:22:33.753929Z","shell.execute_reply.started":"2023-12-03T15:22:28.848902Z","shell.execute_reply":"2023-12-03T15:22:33.753039Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"input_size = 224\ntrain_transform = transforms.Compose([transforms.Resize((input_size,input_size)),\n                                      transforms.RandomHorizontalFlip(),\n                                      transforms.RandomVerticalFlip(),\n                                      transforms.RandomRotation(20),\n                                      transforms.ColorJitter(brightness=0.1, contrast=0.1, hue=0.1),\n                                      transforms.ToTensor(),\n                                      transforms.Normalize(normMean, normStd)])\n# define the transformation of the val images.\nval_transform = transforms.Compose([transforms.Resize((input_size,input_size)),\n                                    transforms.ToTensor(),\n                                    transforms.Normalize(normMean, normStd)])","metadata":{"execution":{"iopub.status.busy":"2023-12-03T15:22:34.907965Z","iopub.execute_input":"2023-12-03T15:22:34.908391Z","iopub.status.idle":"2023-12-03T15:22:34.915201Z","shell.execute_reply.started":"2023-12-03T15:22:34.908360Z","shell.execute_reply":"2023-12-03T15:22:34.914116Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CustomDataset(Dataset):\n    def __init__(self, df, transform=None):\n        self.df = df\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, index):\n        # Load data and get label\n        X = Image.open(self.df['path'][index])\n        y = torch.tensor(int(self.df['cell_type_idx'][index]))\n\n        if self.transform:\n            X = self.transform(X)\n        return X, y","metadata":{"execution":{"iopub.status.busy":"2023-12-03T15:22:35.486741Z","iopub.execute_input":"2023-12-03T15:22:35.487424Z","iopub.status.idle":"2023-12-03T15:22:35.493928Z","shell.execute_reply.started":"2023-12-03T15:22:35.487391Z","shell.execute_reply":"2023-12-03T15:22:35.492896Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define the training set using the table train_df and using our defined transitions (train_transform)\ntraining_set = CustomDataset(df_train, transform=train_transform)\ntrain_loader = DataLoader(training_set, batch_size=32, shuffle=True, num_workers=4)\n# Same for the validation set:\nvalidation_set = CustomDataset(df_val, transform=train_transform)\nval_loader = DataLoader(validation_set, batch_size=32, shuffle=False, num_workers=4)","metadata":{"execution":{"iopub.status.busy":"2023-12-03T15:22:36.777044Z","iopub.execute_input":"2023-12-03T15:22:36.777874Z","iopub.status.idle":"2023-12-03T15:22:36.783557Z","shell.execute_reply.started":"2023-12-03T15:22:36.777841Z","shell.execute_reply":"2023-12-03T15:22:36.782519Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# this function is used during training process, to calculation the loss and accuracy\nclass AverageMeter(object):\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count","metadata":{"execution":{"iopub.status.busy":"2023-12-03T15:22:37.866561Z","iopub.execute_input":"2023-12-03T15:22:37.867442Z","iopub.status.idle":"2023-12-03T15:22:37.873500Z","shell.execute_reply.started":"2023-12-03T15:22:37.867407Z","shell.execute_reply":"2023-12-03T15:22:37.872482Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"total_loss_train, total_acc_train = [],[]\ndef train(train_loader, model, criterion, optimizer, epoch):\n    model.train()\n    train_loss = AverageMeter()\n    train_acc = AverageMeter()\n    curr_iter = (epoch - 1) * len(train_loader)\n    for i, data in enumerate(train_loader):\n        images, labels = data\n        N = images.size(0)\n        # print('image shape:',images.shape, 'label shape',labels.shape)\n        images = Variable(images).to(device)\n        labels = Variable(labels).to(device)\n\n        optimizer.zero_grad()\n        outputs = model(images)\n\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        prediction = outputs.max(1, keepdim=True)[1]\n        train_acc.update(prediction.eq(labels.view_as(prediction)).sum().item()/N)\n        train_loss.update(loss.item())\n        curr_iter += 1\n        if (i + 1) % 100 == 0:\n            print('[epoch %d], [iter %d / %d], [train loss %.5f], [train acc %.5f]' % (\n                epoch, i + 1, len(train_loader), train_loss.avg, train_acc.avg))\n            total_loss_train.append(train_loss.avg)\n            total_acc_train.append(train_acc.avg)\n    return train_loss.avg, train_acc.avg","metadata":{"execution":{"iopub.status.busy":"2023-12-03T15:22:38.809776Z","iopub.execute_input":"2023-12-03T15:22:38.810802Z","iopub.status.idle":"2023-12-03T15:22:38.820899Z","shell.execute_reply.started":"2023-12-03T15:22:38.810766Z","shell.execute_reply":"2023-12-03T15:22:38.819915Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def validate(val_loader, model, criterion, optimizer, epoch):\n    model.eval()\n    val_loss = AverageMeter()\n    val_acc = AverageMeter()\n    with torch.no_grad():\n        for i, data in enumerate(val_loader):\n            images, labels = data\n            N = images.size(0)\n            images = Variable(images).to(device)\n            labels = Variable(labels).to(device)\n\n            outputs = model(images)\n            prediction = outputs.max(1, keepdim=True)[1]\n\n            val_acc.update(prediction.eq(labels.view_as(prediction)).sum().item()/N)\n\n            val_loss.update(criterion(outputs, labels).item())\n\n    print('------------------------------------------------------------')\n    print('[epoch %d], [val loss %.5f], [val acc %.5f]' % (epoch, val_loss.avg, val_acc.avg))\n    print('------------------------------------------------------------')\n    return val_loss.avg, val_acc.avg","metadata":{"execution":{"iopub.status.busy":"2023-12-03T15:22:39.711950Z","iopub.execute_input":"2023-12-03T15:22:39.712812Z","iopub.status.idle":"2023-12-03T15:22:39.720697Z","shell.execute_reply.started":"2023-12-03T15:22:39.712779Z","shell.execute_reply":"2023-12-03T15:22:39.719751Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.to(device)\noptimizer = optim.Adam(model.parameters(), lr=1e-3)\ncriterion = nn.CrossEntropyLoss().to(device)","metadata":{"execution":{"iopub.status.busy":"2023-12-03T15:22:41.023820Z","iopub.execute_input":"2023-12-03T15:22:41.024190Z","iopub.status.idle":"2023-12-03T15:22:44.128806Z","shell.execute_reply.started":"2023-12-03T15:22:41.024162Z","shell.execute_reply":"2023-12-03T15:22:44.127891Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"epoch_num = 10\nbest_val_acc = 0\ntotal_loss_val, total_acc_val = [],[]\nfor epoch in tqdm(range(1, epoch_num+1)):\n    loss_train, acc_train = train(train_loader, model, criterion, optimizer, epoch)\n    loss_val, acc_val = validate(val_loader, model, criterion, optimizer, epoch)\n    total_loss_val.append(loss_val)\n    total_acc_val.append(acc_val)\n    if acc_val > best_val_acc:\n        best_val_acc = acc_val\n        print('*****************************************************')\n        print('best record: [epoch %d], [val loss %.5f], [val acc %.5f]' % (epoch, loss_val, acc_val))\n        print('*****************************************************')","metadata":{"execution":{"iopub.status.busy":"2023-12-03T15:22:45.578869Z","iopub.execute_input":"2023-12-03T15:22:45.579678Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = plt.figure(num = 2)\nfig1 = fig.add_subplot(2,1,1)\nfig2 = fig.add_subplot(2,1,2)\nfig1.plot(total_loss_train, label = 'training loss')\nfig1.plot(total_acc_train, label = 'training accuracy')\nfig2.plot(total_loss_val, label = 'validation loss')\nfig2.plot(total_acc_val, label = 'validation accuracy')\nplt.legend()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    if normalize:\n        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n\n    thresh = cm.max() / 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, cm[i, j],\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.eval()\ny_label = []\ny_predict = []\nwith torch.no_grad():\n    for i, data in enumerate(val_loader):\n        images, labels = data\n        N = images.size(0)\n        images = Variable(images).to(device)\n        outputs = model(images)\n        prediction = outputs.max(1, keepdim=True)[1]\n        y_label.extend(labels.cpu().numpy())\n        y_predict.extend(np.squeeze(prediction.cpu().numpy().T))\n\n# compute the confusion matrix\nconfusion_mtx = confusion_matrix(y_label, y_predict)\n# plot the confusion matrix\nplot_labels = ['akiec', 'bcc', 'bkl', 'df', 'nv', 'vasc','mel']\n# plot_confusion_matrix(confusion_mtx, plot_labels)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import itertools\nplot_confusion_matrix(confusion_mtx, plot_labels)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Generate a classification report\nreport = classification_report(y_label, y_predict, target_names=plot_labels)\nprint(report)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"label_frac_error = 1 - np.diag(confusion_mtx) / np.sum(confusion_mtx, axis=1)\nplt.bar(np.arange(7),label_frac_error)\nplt.xlabel('True Label')\nplt.ylabel('Fraction classified incorrectly')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}